{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78d728de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {\"headers\":{\"Failed\":[\"fail_date\"],\"Succeeded\":[\"success_date\"]},\"rows\":{\"Failed\":[[\"2018-12-28\"],[\"2018-12-29\"],[\"2019-01-04\"],[\"2019-01-05\"]],\"Succeeded\":[[\"2018-12-30\"],[\"2018-12-31\"],[\"2019-01-01\"],[\"2019-01-02\"],[\"2019-01-03\"],[\"2019-01-06\"]]}}\n",
    "pd.DataFrame(d['rows']['Failed'], columns=d['headers']['Failed']).to_csv(\"./Failed.txt\", index=None)\n",
    "pd.DataFrame(d['rows']['Succeeded'], columns=d['headers']['Succeeded']).to_csv(\"./Succeeded.txt\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f521a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"report contiguous dates\").config(\"pyspark.sql.shuffle.partition\", \"4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e531497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| fail_date|\n",
      "+----------+\n",
      "|2018-12-28|\n",
      "|2018-12-29|\n",
      "|2019-01-04|\n",
      "|2019-01-05|\n",
      "+----------+\n",
      "\n",
      "+------------+\n",
      "|success_date|\n",
      "+------------+\n",
      "|  2018-12-30|\n",
      "|  2018-12-31|\n",
      "|  2019-01-01|\n",
      "|  2019-01-02|\n",
      "|  2019-01-03|\n",
      "|  2019-01-06|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = spark.read.option(\"header\", True).csv(\"./Failed.txt\")\n",
    "s = spark.read.option(\"header\", True).csv(\"./Succeeded.txt\")\n",
    "f = f.withColumn(\"fail_date\", f.fail_date.cast(\"date\"))\n",
    "s = s.withColumn(\"success_date\", s.success_date.cast(\"date\"))\n",
    "f.show()\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30b9ae",
   "metadata": {},
   "source": [
    "A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.\n",
    "\n",
    "Write an SQL query to generate a report of period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.\n",
    "\n",
    "period_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.\n",
    "\n",
    "Return the result table ordered by start_date.\n",
    "\n",
    "The query result format is in the following example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4407a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/11 14:08:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/07/11 14:08:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/07/11 14:08:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/07/11 14:08:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n",
      "|period_state|start_time|  end_time|\n",
      "+------------+----------+----------+\n",
      "|   succeeded|2019-01-01|2019-01-03|\n",
      "|   succeeded|2019-01-06|2019-01-06|\n",
      "|      failed|2019-01-04|2019-01-05|\n",
      "+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, date_add, lit, to_date, datediff, col,date_format\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "window_spec = Window.orderBy('fail_date')\n",
    "window_spec_s = Window.orderBy('success_date')\n",
    "# f.select(\"*\", to_date(lit('2019-01-01')).alias('first_date'),row_number().over(window_spec).alias(\"rn\")).select(col(\"fail_date\"), date_add(\"first_date\", 1)).show()\n",
    "\n",
    "grouped = f.where(date_format(\"fail_date\",\"y\")==2019).select(\"*\", date_format(\"fail_date\", \"D\").alias('day_of_year'), row_number().over(window_spec).alias(\"rn\")).select(\"fail_date\", (col('day_of_year')-col('rn')).alias('ind'))\n",
    "max_grouped = grouped.groupBy(\"ind\").agg(F.max(\"fail_date\"))\n",
    "f_range = grouped.groupBy(\"ind\").agg(F.min(\"fail_date\")).join(max_grouped, \"ind\").select(lit(\"failed\").alias(\"period_state\"), \"*\")\n",
    "\n",
    "\n",
    "grouped_s = s.where(date_format(\"success_date\",\"y\")==2019).select(\"*\", date_format(\"success_date\", \"D\").alias('day_of_year'), row_number().over(window_spec_s).alias(\"rn\")).select(\"success_date\", (col('day_of_year')-col('rn')).alias('ind'))\n",
    "max_grouped_s = grouped_s.groupBy(\"ind\").agg(F.max(\"success_date\"))\n",
    "grouped_s.groupBy(\"ind\").agg(F.min(\"success_date\")).join(max_grouped_s, \"ind\").withColumnRenamed(\"min(success_date)\",\"start_time\").withColumnRenamed(\"max(success_date)\",\"end_time\").select(lit(\"succeeded\").alias(\"period_state\"), \"*\").unionAll(f_range).select(\"period_state\", \"start_time\", 'end_time').show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
