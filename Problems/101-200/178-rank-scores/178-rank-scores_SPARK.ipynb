{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f21bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# {\"headers\": {\"Scores\": [\"id\", \"score\"]}, \"rows\": {\"Scores\": [[1, 3.50], [2, 3.65], [3, 4.00], [4, 3.85], [5, 4.00], [6, 3.65]]}}\n",
    "pd.DataFrame([[1, 3.50], [2, 3.65], [3, 4.00], [4, 3.85], [5, 4.00], [6, 3.65]], columns=[\"id\", \"score\"]).to_csv(\"./Scores.txt\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"178 rank scores\").config(\"pyspark.sql.shuffle.partition\", \"4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7322b355",
   "metadata": {},
   "source": [
    "Write an SQL query to rank the scores. The ranking should be calculated according to the following rules:\n",
    "\n",
    "The scores should be ranked from the highest to the lowest.\n",
    "If there is a tie between two scores, both should have the same ranking.\n",
    "After a tie, the next ranking number should be the next consecutive integer value. In other words, there should be no holes between ranks.\n",
    "Return the result table ordered by score in descending order.\n",
    "\n",
    "The query result format is in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc19cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|score|\n",
      "+---+-----+\n",
      "|  1|  3.5|\n",
      "|  2| 3.65|\n",
      "|  3|  4.0|\n",
      "|  4| 3.85|\n",
      "|  5|  4.0|\n",
      "|  6| 3.65|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = spark.read.option(\"header\", True).csv(\"./Scores.txt\")\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4397e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/09 11:26:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| id|score|rank|\n",
      "+---+-----+----+\n",
      "|  3|  4.0|   1|\n",
      "|  5|  4.0|   1|\n",
      "|  4| 3.85|   2|\n",
      "|  2| 3.65|   3|\n",
      "|  6| 3.65|   3|\n",
      "|  1|  3.5|   4|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank,col\n",
    "Word_spec = Window.orderBy(col(\"score\").desc())\n",
    "scores = scores.withColumn(\"rank\", dense_rank().over(Word_spec))\n",
    "scores.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
