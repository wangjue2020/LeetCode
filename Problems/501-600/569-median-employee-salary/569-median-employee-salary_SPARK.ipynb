{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8c20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {\"headers\": {\"Employee\": [\"id\", \"company\", \"salary\"]}, \"rows\": {\"Employee\": [[1, \"A\", 2341],[2, \"A\", 341],[3, \"A\", 15],[4, \"A\", 15314],[5, \"A\", 451],[6, \"A\", 513],[7, \"B\", 15],[8, \"B\", 13],[9, \"B\", 1154],[10, \"B\", 1345],[11, \"B\", 1221],[12, \"B\", 234],[13, \"C\", 2345],[14, \"C\", 2645],[15, \"C\", 2645],[16, \"C\", 2652],[17, \"C\", 65]]}}\n",
    "pd.DataFrame(d['rows']['Employee'], columns = d['headers']['Employee']).to_csv(\"./Employee.txt\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79446a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"median employee salary\").config(\"pyspark.sql.shuffle.partition\", \"4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ec81704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|company|salary|\n",
      "+---+-------+------+\n",
      "|  1|      A|  2341|\n",
      "|  2|      A|   341|\n",
      "|  3|      A|    15|\n",
      "|  4|      A| 15314|\n",
      "|  5|      A|   451|\n",
      "|  6|      A|   513|\n",
      "|  7|      B|    15|\n",
      "|  8|      B|    13|\n",
      "|  9|      B|  1154|\n",
      "| 10|      B|  1345|\n",
      "| 11|      B|  1221|\n",
      "| 12|      B|   234|\n",
      "| 13|      C|  2345|\n",
      "| 14|      C|  2645|\n",
      "| 15|      C|  2645|\n",
      "| 16|      C|  2652|\n",
      "| 17|      C|    65|\n",
      "+---+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e = spark.read.option(\"header\", True).csv(\"./Employee.txt\")\n",
    "e.show()\n",
    "e = e.withColumn(\"salary\", e.salary.cast('int'))\n",
    "e = e.withColumn(\"id\", e.id.cast('int'))\n",
    "e.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b0f61",
   "metadata": {},
   "source": [
    "Write an SQL query to find the median salary of each company.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The query result format is in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7830693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+\n",
      "|company| fi| se|\n",
      "+-------+---+---+\n",
      "|      B|3.0|4.0|\n",
      "|      C|3.0|3.0|\n",
      "|      A|3.0|4.0|\n",
      "+-------+---+---+\n",
      "\n",
      "+---+-------+------+---+\n",
      "| id|company|salary|row|\n",
      "+---+-------+------+---+\n",
      "|  8|      B|    13|  1|\n",
      "|  7|      B|    15|  2|\n",
      "| 12|      B|   234|  3|\n",
      "|  9|      B|  1154|  4|\n",
      "| 11|      B|  1221|  5|\n",
      "| 10|      B|  1345|  6|\n",
      "| 17|      C|    65|  1|\n",
      "| 13|      C|  2345|  2|\n",
      "| 14|      C|  2645|  3|\n",
      "| 15|      C|  2645|  4|\n",
      "| 16|      C|  2652|  5|\n",
      "|  3|      A|    15|  1|\n",
      "|  2|      A|   341|  2|\n",
      "|  5|      A|   451|  3|\n",
      "|  6|      A|   513|  4|\n",
      "|  1|      A|  2341|  5|\n",
      "|  4|      A| 15314|  6|\n",
      "+---+-------+------+---+\n",
      "\n",
      "+-------+---+------+---+---+---+\n",
      "|company| id|salary|row| fi| se|\n",
      "+-------+---+------+---+---+---+\n",
      "|      B|  9|  1154|  4|3.0|4.0|\n",
      "|      B| 12|   234|  3|3.0|4.0|\n",
      "|      C| 14|  2645|  3|3.0|3.0|\n",
      "|      A|  6|   513|  4|3.0|4.0|\n",
      "|      A|  5|   451|  3|3.0|4.0|\n",
      "+-------+---+------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "median = e.groupBy(\"company\").count().withColumnRenamed('count', 'cnt').select('company', round(col('cnt')/2).alias(\"fi\"), round((col('cnt')+1)/2).alias(\"se\"))\n",
    "median.show()\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "Window_spec = Window.partitionBy('company').orderBy('salary')\n",
    "e.select('*', row_number().over(Window_spec).alias(\"row\")).show()\n",
    "e.select('*', row_number().over(Window_spec).alias(\"row\")).join(median,'company').where(col('row').between(col('fi'), col('se'))).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
