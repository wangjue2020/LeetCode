{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeabc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {\"headers\":{\"Activity\":[\"player_id\",\"device_id\",\"event_date\",\"games_played\"]},\"rows\":{\"Activity\":[[1,2,\"2016-03-01\",5],[1,2,\"2016-03-02\",6],[2,3,\"2017-06-25\",1],[3,1,\"2016-03-01\",0],[3,4,\"2018-07-03\",5]]}}\n",
    "pd.DataFrame(d['rows']['Activity'], columns=d['headers']['Activity']).to_csv(\"./Activity.txt\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f0a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"game play analysis v\").config(\"pyspark.sql.shuffle.partition\", \"4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b981c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- games_played: integer (nullable = true)\n",
      "\n",
      "+---------+---------+----------+------------+\n",
      "|player_id|device_id|event_date|games_played|\n",
      "+---------+---------+----------+------------+\n",
      "|        1|        2|2016-03-01|           5|\n",
      "|        1|        2|2016-03-02|           6|\n",
      "|        2|        3|2017-06-25|           1|\n",
      "|        3|        1|2016-03-01|           0|\n",
      "|        3|        4|2018-07-03|           5|\n",
      "+---------+---------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = spark.read.option(\"header\", True).csv(\"./Activity.txt\")\n",
    "a = a.withColumn(\"player_id\", a.player_id.cast('int')).withColumn(\"device_id\", a.device_id.cast('int')).withColumn(\"games_played\", a.games_played.cast('int')).withColumn(\"event_date\", a.event_date.cast('date'))\n",
    "a.printSchema()\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8aebbb",
   "metadata": {},
   "source": [
    "The install date of a player is the first login day of that player.\n",
    "\n",
    "We define day one retention of some date x to be the number of players whose install date is x and they logged back in on the day right after x, divided by the number of players whose install date is x, rounded to 2 decimal places.\n",
    "\n",
    "Write an SQL query to report for each install date, the number of players that installed the game on that day, and the day one retention.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The query result format is in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4da24ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------+\n",
      "|install dt|installs|Day1_retention|\n",
      "+----------+--------+--------------+\n",
      "|2016-03-01|       2|           0.5|\n",
      "|2017-06-25|       1|           0.0|\n",
      "+----------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "def addAlias(df, n):\n",
    "    for i in df.columns:\n",
    "        df = df.withColumnRenamed(i, f'{i}_{n}')\n",
    "    return df\n",
    "\n",
    "a1 = addAlias(a,1)\n",
    "\n",
    "first_login = a.groupBy('player_id')\\\n",
    "            .agg(F.min('event_date'))\\\n",
    "            .withColumnRenamed(\"min(event_date)\", \"first_login\")\n",
    "\n",
    "installs = first_login.join(a, \"player_id\", \"leftouter\")\\\n",
    "            .where(a.event_date==first_login.first_login)\n",
    "\n",
    "retention = installs.join(a1, a1.player_id_1==F.col('player_id') , \"leftouter\")\\\n",
    "            .where(F.datediff(a1.event_date_1, F.col('event_date'))==1)\\\n",
    "            .groupBy(\"first_login\").count()\n",
    "\n",
    "installs.groupBy(\"first_login\").count().withColumnRenamed(\"count\",\"installs\").join(retention, \"first_login\", \"leftouter\").withColumnRenamed(\"first_login\",\"install dt\").select(\"install dt\", \"installs\", (F.col(\"count\")/F.col('installs')).alias(\"Day1_retention\")).na.fill(0).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
