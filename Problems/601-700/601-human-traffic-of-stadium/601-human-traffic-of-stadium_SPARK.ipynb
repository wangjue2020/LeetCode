{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3885418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {\"headers\": {\"Stadium\": [\"id\", \"visit_date\", \"people\"]}, \"rows\": {\"Stadium\": [[1, \"2017-01-01\", 10], [2, \"2017-01-02\", 109], [3, \"2017-01-03\", 150], [4, \"2017-01-04\", 99], [5, \"2017-01-05\", 145], [6, \"2017-01-06\", 1455], [7, \"2017-01-07\", 199], [8, \"2017-01-09\", 188]]}}\n",
    "pd.DataFrame(d['rows']['Stadium'], columns=d['headers']['Stadium']).to_csv(\"./Stadium.txt\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5b2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"human traffic of stadium\").config(\"pyspark.sql.shuffle.partition\",\"4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d241d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id|visit_date|people|\n",
      "+---+----------+------+\n",
      "|  1|2017-01-01|    10|\n",
      "|  2|2017-01-02|   109|\n",
      "|  3|2017-01-03|   150|\n",
      "|  4|2017-01-04|    99|\n",
      "|  5|2017-01-05|   145|\n",
      "|  6|2017-01-06|  1455|\n",
      "|  7|2017-01-07|   199|\n",
      "|  8|2017-01-09|   188|\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = spark.read.option(\"header\", True).csv(\"./Stadium.txt\")\n",
    "s = s.withColumn(\"id\", s.id.cast('int')).withColumn('visit_date', s.visit_date.cast('date')).withColumn('people', s.people.cast('int'))\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4c4776",
   "metadata": {},
   "source": [
    "Write an SQL query to display the records with three or more rows with consecutive id's, and the number of people is greater than or equal to 100 for each.\n",
    "\n",
    "Return the result table ordered by visit_date in ascending order.\n",
    "\n",
    "The query result format is in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a898a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/10 11:42:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/07/10 11:42:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id|visit_date|people|\n",
      "+---+----------+------+\n",
      "|  5|2017-01-05|   145|\n",
      "|  6|2017-01-06|  1455|\n",
      "|  7|2017-01-07|   199|\n",
      "|  8|2017-01-09|   188|\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "Window_spec = Window.orderBy('id')\n",
    "tmp = s.where('people>=100').select(\"*\", (s.id-row_number().over(Window_spec)).alias(\"row\"))\n",
    "tmp.groupBy('row').count().where('count>=3').join(tmp, 'row', 'inner').select('id','visit_date','people').orderBy('visit_date', ascending=True).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
