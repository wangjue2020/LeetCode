{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292a5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {\"headers\":{\"Users\":[\"user_id\",\"join_date\",\"favorite_brand\"],\"Orders\":[\"order_id\",\"order_date\",\"item_id\",\"buyer_id\",\"seller_id\"],\"Items\":[\"item_id\",\"item_brand\"]},\"rows\":{\"Users\":[[1,\"2019-01-01\",\"Lenovo\"],[2,\"2019-02-09\",\"Samsung\"],[3,\"2019-01-19\",\"LG\"],[4,\"2019-05-21\",\"HP\"]],\"Orders\":[[1,\"2019-08-01\",4,1,2],[2,\"2019-08-02\",2,1,3],[3,\"2019-08-03\",3,2,3],[4,\"2019-08-04\",1,4,2],[5,\"2019-08-04\",1,3,4],[6,\"2019-08-05\",2,2,4]],\"Items\":[[1,\"Samsung\"],[2,\"Lenovo\"],[3,\"LG\"],[4,\"HP\"]]}}\n",
    "pd.DataFrame(d['rows']['Orders'], columns=d['headers']['Orders']).to_csv(\"./Orders.txt\", index=None)\n",
    "pd.DataFrame(d['rows']['Users'], columns=d['headers']['Users']).to_csv(\"./Users.txt\", index=None)\n",
    "pd.DataFrame(d['rows']['Items'], columns=d['headers']['Items']).to_csv(\"./Items.txt\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34109a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"martket analysis II\").config(\"pyspark.sql.shuffle.partition\", \"4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fb68c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+--------+---------+\n",
      "|order_id|order_date|item_id|buyer_id|seller_id|\n",
      "+--------+----------+-------+--------+---------+\n",
      "|       1|2019-08-01|      4|       1|        2|\n",
      "|       2|2019-08-02|      2|       1|        3|\n",
      "|       3|2019-08-03|      3|       2|        3|\n",
      "|       4|2019-08-04|      1|       4|        2|\n",
      "|       5|2019-08-04|      1|       3|        4|\n",
      "|       6|2019-08-05|      2|       2|        4|\n",
      "+--------+----------+-------+--------+---------+\n",
      "\n",
      "+-------+----------+--------------+\n",
      "|user_id| join_date|favorite_brand|\n",
      "+-------+----------+--------------+\n",
      "|      1|2019-01-01|        Lenovo|\n",
      "|      2|2019-02-09|       Samsung|\n",
      "|      3|2019-01-19|            LG|\n",
      "|      4|2019-05-21|            HP|\n",
      "+-------+----------+--------------+\n",
      "\n",
      "+-------+----------+\n",
      "|item_id|item_brand|\n",
      "+-------+----------+\n",
      "|      1|   Samsung|\n",
      "|      2|    Lenovo|\n",
      "|      3|        LG|\n",
      "|      4|        HP|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o = spark.read.option(\"header\", True).csv(\"./Orders.txt\")\n",
    "u = spark.read.option(\"header\", True).csv(\"./Users.txt\")\n",
    "i = spark.read.option(\"header\", True).csv(\"./Items.txt\")\n",
    "o.show()\n",
    "u.show()\n",
    "i.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202c9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = o.withColumn(\"order_id\", o.order_id.cast('int')).withColumn(\"seller_id\", o.seller_id.cast('int')).withColumn(\"item_id\", o.item_id.cast('int')).withColumn(\"buyer_id\", o.buyer_id.cast('int')).withColumn(\"order_date\", o.order_date.cast('date'))\n",
    "u = u.withColumn(\"user_id\", u.user_id.cast('int')).withColumn(\"join_date\", u.join_date.cast('date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ed97e",
   "metadata": {},
   "source": [
    "Write an SQL query to find for each user whether the brand of the second item (by date) they sold is their favorite brand. If a user sold less than two items, report the answer for that user as no. It is guaranteed that no seller sold more than one item on a day.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The query result format is in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afb7890d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:=========================================>            (153 + 8) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|user_id|2nd_favori_brand|\n",
      "+-------+----------------+\n",
      "|      1|              no|\n",
      "|      2|             yes|\n",
      "|      3|             yes|\n",
      "|      4|              no|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number,col,when\n",
    "window_spec = Window.partitionBy(\"seller_id\").orderBy(\"order_date\")\n",
    "second_item = o.join(i, \"item_id\", \"inner\").select(\"order_date\", \"seller_id\", \"item_brand\", row_number().over(window_spec).alias(\"rn\")).where(col('rn')==2)\n",
    "u.join(second_item, second_item.seller_id==u.user_id, \"leftouter\").select(\"user_id\", when(u.favorite_brand==second_item.item_brand, \"yes\").otherwise(\"no\").alias(\"2nd_favori_brand\")).na.fill(\"no\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
